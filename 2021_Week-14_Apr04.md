# ARTS 2021W14

## Algorithm

### Unique Binary Search Trees II

[Leetcode 95](https://leetcode.com/problems/unique-binary-search-trees-ii/)

首先，BST 的特性是右>中>左，因此构建树的过程为先选取中间节点，然后递归地构建左子树和右子树。

因为递归的过程会有一些重复计算，因此可以加入记忆数组，避免重复计算。

需要注意的是，部分节点会出现没有左子树或右子树的情况，这时辅助函数 需要返回 ```[None]``` 而不是 ```[]```。

```python
from functools import cache

class Solution:
    def generateTrees(self, n: int) -> List[TreeNode]:
        if n == 0:
            return []
        return self.helper(1, n)
    
    @cache
    def helper(self, start, end):
        if start > end:
            return [ None ]
        res = []
        for i in range(start, end+1):
            left = self.helper(start, i-1)
            right = self.helper(i+1, end)
            
            for lnode in left:
                for rnode in right:
                    root = TreeNode(i)
                    root.left = lnode
                    root.right = rnode
                    res.append(root)
        return res
```

### Palindrome Partitioning

[Palindrome Partitioning - LeetCode](https://leetcode.com/problems/palindrome-partitioning/)

因为需要找到所有子串，因此需要通过 DFS 遍历全部分支。需要以下三个函数：

1. 入口函数
2. 用于 DFS 的辅助函数
3. 用于判断是否为回文子串的辅助函数

其中，回文子串辅助函数可以加入备忘录模式，减少重复计算。

需要注意两个点：

1. 列表索引：添加一组新结果时要调用拷贝函数，否则在 res 数组里新增的只是 out 的索引。
2. 开闭区间：子串入队时是左闭右闭区间。

```python
from functools import lru_cache

class Solution:
    def partition(self, s: str) -> List[List[str]]:
        n = len(s)
        out = []
        res = []

        @lru_cache(maxsize=None)
        def is_palindrome(start, end):
            while start < end:
                if s[start] != s[end]:
                    return False
                start += 1
                end -= 1
            return True

        def helper(start):
            if start >= n:
                # 注意：添加要调用 .copy 添加一个副本，否则会添加索引
                res.append(out.copy())
                return
            
            for i in range(start, n):
                if not is_palindrome(start, i):
                    continue
                out.append(s[start:i+1]) # 注意开闭区间，is_palindrome 是左闭右闭判断的，循环条件是左闭右开
                helper(i+1)
                out.pop(-1)

        helper(0)
        return res
```

## Review

[阿里技术：一文读懂云上DevOps能力体系](https://zhuanlan.zhihu.com/p/349221546)

文章从运维的职能出发，列举了自动化运维在不同级别需要达到的水平，以及如何向高度自动化进行迁移。

详细内容如下：

1、类比自动驾驶的等级，列出了自动化运维金字塔

这个观点很有趣。文章指明了不同等级的自动化运维的可以实现的功能，方便在项目 Scoping 的过程中确保合理的预期。

- Level1: 手动运维。直接登录服务器操作或通过云服务商的 UI 进行运维操作。
- Level2: 半手工/半自动化运维，自动化程度低于 30%。运维可以通过 CLI 操作。
- Level3: 高度自动化，自动化程度达到 50% 左右。会通过云平台 SDK/API 进行日常运维工作；同时开发运维系统，和内部系统紧耦合。
- Level4: 标准化自动化，自动化程度超过 90%。运维系统已经具备平台化、模板化和代码化的能力，可以根据自身需求定制化开发运维系统。
- Level5: AIOps，自动化程度达到 100%。不再需要值班人员，生产力完全解放。

2、介绍了 DevOps 的进阶模式

模板化、代码化是构建 AIOps 的基础。

3、DevOps 的核心：CI/CD、IaC

4、完整的 DevOps 应该具备哪些能力

5、DevOps 落地的阻碍：如何和财务流程实现平衡

评论：在 DevOps 迁移的过程中，时常会遇到开发人员对于自动化运维有不合理预期的问题。通过这个自动化运维金字塔，可以很好地管理开发人员对“自动化”的预期，从而逐步推进 DevOps 的迁移。

## Tips

Docker Push 大镜像时报 ```unauthorized: authentication required``` 错误的处理。

由于 Docker Registry 是基于 Token 验证，因此在推镜像时通常会有一个超时时间，[参见此 issue](https://gitlab.com/gitlab-org/gitlab-foss/-/issues/19867#note_15074443)。

所以，我们在配置 Gitlab/Harbor Registry 时需要根据服务器的带宽以及预期的最大镜像选择一个适当的 Token 超时时间。

## Share: 通过 Docker 搭建 Ceph 测试集群环境

本文分享如何利用 Docker 在本地建立一个 3 Mon + 3 OSD 的 Ceph 集群。

### 准备工作

首先需要拉取较旧的镜像。

由于新版的 Ceph 有些特性（如 ceph-disk 的挂载方式等）有变更，网上的教程大多不能直接跑起来了。

经过测试，```jewel```是可以顺利跑起来的版本：

```shell
export CEPH_IMAGE=ceph/daemon:v3.1.0-stable-3.1-jewel-centos-7-x86_64
docker pull $CEPH_IMAGE
```

然后我们需要建立一个 Ceph 子网。

通过以下命令创建一个 ```172.18.0.0/16```，名为 ```ceph-test``` 的子网。

```shell
docker network create --subnet=172.18.0.0/16 ceph-test
```

【可选】最后是创建虚拟磁盘。

由于 Ceph 需要独占磁盘（将磁盘格式化为 ```Ceph OSD``` 与```Ceph Journal``` 类型），因此我们在测试环境里可以采用虚拟磁盘的形式。

通过以下命令创建一个虚拟磁盘文件，然后利用 ```losetup``` 把磁盘文件虚拟为区块设备。

```shell
dd if=/dev/zero of=node3-hdd.img bs=1M seek=50000 count=0
sudo losetup -Pf --show node3-hdd.img
```

```losetup``` 命令执行成功后会返回对应的设备路径，如：

```
/dev/loop16
```

如果后续操作遇到问题，可以通过以下指令先创建一个分区表。格外需要留意路径，以免格式化错误的磁盘。

```
sudo mkfs.xfs /dev/loop16
```

### 搭建集群

一个最小可用集群需要 1Mon+3OSD，在本例中选择的配置为 3Mon+3OSD。

#### 创建 Mon 节点

创建 Ceph 集群的第一步是创建监视器节点。

首先，通过以下命令可以创建一个 Mon 节点。

```shell
docker run -d --name ceph-node1 \
        --net=ceph-test \
        --ip 172.18.0.2 \
        -v $NODE1_DIR/etc:/etc/ceph \
        -v $NODE1_DIR/var:/var/lib/ceph/ \
        -e MON_IP=172.18.0.2,172.18.0.3,172.18.0.4 \
        -e CEPH_PUBLIC_NETWORK=172.18.0.0/16 \
        $CEPH_IMAGE mon
```

通过以下命令查看节点状态：

```shell
docker exec -it ceph-node1 ceph -s
```

可以设置一个 Shell 别名，方便后续操作

```
alias ceph="docker exec -it ceph-node1 ceph"
```

创建好第一个节点后，我们需要另外创建两个目录，把节点一的配置文件 ```etc/*``` 和 ```var/bootstrap*``` 复制到新目录里，作为新节点的配置文件。

然后修改前述节点的启动命令，创建另外两个节点。

#### 创建 OSD 节点

OSD 节点为实际存储数据的节点，OSD 守护进程会检查自身状态、其他 OSD 状态，并报告给监视器们。

首先，我们需要生成客户端密钥：

```
docker exec ceph-node1 ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring
```

通过以下命令可以创建 OSD 节点：

```
docker run -d --name ceph-node1-osd \
        --net=ceph-test \
        --ip 172.18.0.5 \
        -v $NODE1_DIR/etc:/etc/ceph \
        -v $NODE1_DIR/var:/var/lib/ceph/ \
        -v /dev/:/dev/ \
        --privileged=true \
        -e OSD_ID=0 \
        -e OSD_DEVICE=/dev/loop16 \
        -e OSD_FORCE_ZAP=1 \
        $CEPH_IMAGE osd_ceph_disk
```

上述脚本中，为模仿 3 虚拟机的部署方式，直接采用了对应 Mon 节点的配置文件。但是节点 IP 不能重复占用，因此给了三个新的 IP。

3 OSD 节点完成启动后，可以通过 ```ceph -s``` 指令看到如下输出：

```
     osdmap e19: 3 osds: 3 up, 3 in
            flags sortbitwise,require_jewel_osds
      pgmap v1001: 80 pgs, 3 pools, 1932 MB data, 505 objects
            6124 MB used, 140 GB / 146 GB avail
                  80 active+clean
```

至此我们完成了最基本的 Ceph 集群搭建工作。

### CephFS 挂载

#### 创建 MDS 节点

如果需要 CephFS 功能，我们还需要启动一个 MDS 节点，用于管理文件系统的元信息（Metadata）。

同样，需要先新建一个目录并拷贝节点 1 的配置文件，然后通过如下命令启动 MDS 节点：

```
docker run -d --net=ceph-test --name ceph-mds1 \
    --ip 172.18.0.20 \
    -v $MDS1_DIR/etc:/etc/ceph \
    -v $MDS1_DIR/var:/var/lib/ceph/ \
    -e CEPHFS_CREATE=1 \
    $CEPH_IMAGE mds
```

启动完成后，在 ```ceph -s``` 可以看到如下输出：

```
monmap e3: 3 mons at {0c8d26cc899b=172.18.0.3:6789/0,4e985a85135a=172.18.0.4:6789/0,bde0c2d409d9=172.18.0.2:6789/0}
            election epoch 8, quorum 0,1,2 bde0c2d409d9,0c8d26cc899b,4e985a85135a
```

#### 挂载 CephFS

上述脚本中的 ```-e CEPHFS_CREATE=1``` 环境变量会自动创建一个 CephFS 的存储池。

通过 ```ceph fs ls``` 可以查看文件系统信息。

为了测试方便，我们可以直接通过 ```client.admin``` 的密钥挂载文件系统。

```
ceph auth get client.admin
```

```key``` 对应的值就是 Base64 编码后的密钥。

```
exported keyring for client.admin
[client.admin]
	key = ABCDEFG==
	auid = 0
	caps mds = "allow"
	caps mgr = "allow *"
	caps mon = "allow *"
	caps osd = "allow *"
```

然后我们就可以在本地挂载相应的目录了：

```
sudo mount -v -t ceph 172.18.0.2:6789,172.18.0.3:6789,172.18.0.4:6789:/ /home/cephfs -o name=admin,secret=ABCDEFG==
```

